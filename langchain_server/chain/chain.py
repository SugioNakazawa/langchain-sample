"""
LangChain ãƒã‚§ãƒ¼ãƒ³ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ ã‚µãƒ³ãƒ—ãƒ«é›†

ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã§ã¯ã€LangChainã®æ§˜ã€…ãªãƒã‚§ãƒ¼ãƒ³ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å®Ÿè£…ã—ã¾ã™ï¼š
1. Simple Chain - åŸºæœ¬çš„ãªå˜ä¸€ãƒã‚§ãƒ¼ãƒ³
2. Sequential Chain - è¤‡æ•°ã‚¹ãƒ†ãƒƒãƒ—ã®é †æ¬¡å®Ÿè¡Œ
3. Router Chain - æ¡ä»¶åˆ†å²
4. Transform Chain - ãƒ‡ãƒ¼ã‚¿å¤‰æ›
5. LCEL (LangChain Expression Language) - æœ€æ–°ã®æ¨å¥¨æ–¹æ³•
6. RAG Chain - æ¤œç´¢æ‹¡å¼µç”Ÿæˆ
"""

import asyncio
from typing import Dict, List, Any
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate, PromptTemplate
from langchain_core.output_parsers import StrOutputParser, JsonOutputParser
from langchain_core.runnables import RunnablePassthrough, RunnableLambda, RunnableParallel
from pydantic import BaseModel, Field

# ===== å…±é€šè¨­å®š =====
def get_llm(streaming: bool = False, temperature: float = 0.7):
    """LLMã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾—"""
    return ChatOpenAI(
        openai_api_base="http://localhost:11434/v1",
        streaming=streaming,
        temperature=temperature,
        openai_api_key="EMPTY",
        model="qwen3:8b"
    )

# ===== 1. Simple Chain - åŸºæœ¬çš„ãªå˜ä¸€ãƒã‚§ãƒ¼ãƒ³ =====
async def simple_chain_example():
    """æœ€ã‚‚ã‚·ãƒ³ãƒ—ãƒ«ãªãƒã‚§ãƒ¼ãƒ³ã®ä¾‹"""
    print("\n" + "="*60)
    print("1ï¸âƒ£  Simple Chain Example")
    print("="*60)
    
    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
    prompt = ChatPromptTemplate.from_template(
        "ã‚ãªãŸã¯ãƒ—ãƒ­ã®ç¿»è¨³è€…ã§ã™ã€‚ä»¥ä¸‹ã®æ–‡ç« ã‚’{target_lang}ã«ç¿»è¨³ã—ã¦ãã ã•ã„ï¼š\n\n{text}"
    )
    question = "ä»Šæ—¥ã¯è‰¯ã„å¤©æ°—ã§ã™ã­ã€‚æ•£æ­©ã«è¡Œãã¾ã—ã‚‡ã†ã€‚"
    
    # ãƒã‚§ãƒ¼ãƒ³æ§‹ç¯‰ (LCELæ–¹å¼)
    chain = prompt | get_llm() | StrOutputParser()
    
    # å®Ÿè¡Œ
    result = await chain.ainvoke({
        "target_lang": "è‹±èª",
        "text": question
    })
    print(f"\nè³ªå•: {question}")
    print(f"\nç¿»è¨³çµæœ: {result}")
    return result

# ===== 2. Sequential Chain - è¤‡æ•°ã‚¹ãƒ†ãƒƒãƒ—ã®é †æ¬¡å®Ÿè¡Œ =====
async def sequential_chain_example():
    """è¤‡æ•°ã®ãƒã‚§ãƒ¼ãƒ³ã‚’é †æ¬¡å®Ÿè¡Œã™ã‚‹ä¾‹"""
    print("\n" + "="*60)
    print("2ï¸âƒ£  Sequential Chain Example")
    print("="*60)
    
    # ã‚¹ãƒ†ãƒƒãƒ—1: ãƒˆãƒ”ãƒƒã‚¯ç”Ÿæˆ
    topic_prompt = ChatPromptTemplate.from_template(
        "ä»¥ä¸‹ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‹ã‚‰ã€ãƒ–ãƒ­ã‚°è¨˜äº‹ã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’1ã¤ææ¡ˆã—ã¦ãã ã•ã„ï¼š{keywords}"
    )
    
    # ã‚¹ãƒ†ãƒƒãƒ—2: æ¦‚è¦ä½œæˆ
    outline_prompt = ChatPromptTemplate.from_template(
        "ä»¥ä¸‹ã®ãƒ–ãƒ­ã‚°ã‚¿ã‚¤ãƒˆãƒ«ã«åŸºã¥ã„ã¦ã€3ã¤ã®è¦‹å‡ºã—ã‚’ç®‡æ¡æ›¸ãã§ä½œæˆã—ã¦ãã ã•ã„ï¼š\n\n{title}"
    )
    
    # ã‚¹ãƒ†ãƒƒãƒ—3: æœ¬æ–‡ä½œæˆ
    content_prompt = ChatPromptTemplate.from_template(
        "ä»¥ä¸‹ã®ã‚¿ã‚¤ãƒˆãƒ«ã¨è¦‹å‡ºã—ã«åŸºã¥ã„ã¦ã€200æ–‡å­—ç¨‹åº¦ã®å°å…¥æ–‡ã‚’æ›¸ã„ã¦ãã ã•ã„ï¼š\n\nã‚¿ã‚¤ãƒˆãƒ«: {title}\nè¦‹å‡ºã—:\n{outline}"
    )
    
    # LCEL ã§é †æ¬¡å®Ÿè¡Œãƒã‚§ãƒ¼ãƒ³ã‚’æ§‹ç¯‰
    chain = (
        {"keywords": RunnablePassthrough()}
        | RunnableParallel(
            title=(topic_prompt | get_llm() | StrOutputParser()),
            keywords=RunnablePassthrough()
        )
        | RunnableParallel(
            title=lambda x: x["title"],
            outline=(
                RunnableLambda(lambda x: {"title": x["title"]})
                | outline_prompt
                | get_llm()
                | StrOutputParser()
            )
        )
        | RunnableParallel(
            title=lambda x: x["title"],
            outline=lambda x: x["outline"],
            content=(
                RunnableLambda(lambda x: {"title": x["title"], "outline": x["outline"]})
                | content_prompt
                | get_llm()
                | StrOutputParser()
            )
        )
    )
    
    result = await chain.ainvoke("AI, æ©Ÿæ¢°å­¦ç¿’, Python")
    
    print(f"\nğŸ“ ã‚¿ã‚¤ãƒˆãƒ«:\n{result['title']}")
    print(f"\nğŸ“‹ è¦‹å‡ºã—:\n{result['outline']}")
    print(f"\nâœï¸  å°å…¥æ–‡:\n{result['content']}")
    
    return result

# ===== 3. Router Chain - æ¡ä»¶åˆ†å² =====
async def router_chain_example():
    """å…¥åŠ›å†…å®¹ã«å¿œã˜ã¦å‡¦ç†ã‚’åˆ†å²ã™ã‚‹ä¾‹"""
    print("\n" + "="*60)
    print("3ï¸âƒ£  Router Chain Example")
    print("="*60)
    
    # åˆ†é¡ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
    classifier_prompt = ChatPromptTemplate.from_template(
        """ä»¥ä¸‹ã®è³ªå•ã‚’åˆ†é¡ã—ã¦ãã ã•ã„ã€‚ã€ŒæŠ€è¡“ã€ã€Œãƒ“ã‚¸ãƒã‚¹ã€ã€Œæ—¥å¸¸ã€ã®ã„ãšã‚Œã‹ã§ç­”ãˆã¦ãã ã•ã„ã€‚
        
è³ªå•: {question}

åˆ†é¡:"""
    )
    
    # ã‚«ãƒ†ã‚´ãƒªåˆ¥ã®å¿œç­”ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
    tech_prompt = ChatPromptTemplate.from_template(
        "æŠ€è¡“çš„ãªè¦³ç‚¹ã‹ã‚‰è©³ã—ãèª¬æ˜ã—ã¾ã™ï¼š{question}"
    )
    
    business_prompt = ChatPromptTemplate.from_template(
        "ãƒ“ã‚¸ãƒã‚¹ã®è¦–ç‚¹ã§å®Ÿç”¨çš„ã«èª¬æ˜ã—ã¾ã™ï¼š{question}"
    )
    
    casual_prompt = ChatPromptTemplate.from_template(
        "åˆ†ã‹ã‚Šã‚„ã™ãæ—¥å¸¸çš„ãªè¨€è‘‰ã§èª¬æ˜ã—ã¾ã™ï¼š{question}"
    )
    
    # ãƒ«ãƒ¼ã‚¿ãƒ¼é–¢æ•°
    def route_question(input_dict: Dict) -> Any:
        category = input_dict["category"].strip().lower()
        question = input_dict["question"]
        
        if "æŠ€è¡“" in category:
            return tech_prompt | get_llm() | StrOutputParser()
        elif "ãƒ“ã‚¸ãƒã‚¹" in category:
            return business_prompt | get_llm() | StrOutputParser()
        else:
            return casual_prompt | get_llm() | StrOutputParser()
    
    # ãƒ«ãƒ¼ã‚¿ãƒ¼ãƒã‚§ãƒ¼ãƒ³æ§‹ç¯‰
    chain = (
        RunnableParallel(
            question=RunnablePassthrough(),
            category=(classifier_prompt | get_llm() | StrOutputParser())
        )
        | RunnableLambda(route_question)
    )
    
    # ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹
    questions = [
        "Pythonã®éåŒæœŸãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã¨ã¯ï¼Ÿ",
        "ã‚¹ã‚¿ãƒ¼ãƒˆã‚¢ãƒƒãƒ—ã®è³‡é‡‘èª¿é”æ–¹æ³•ã¯ï¼Ÿ",
        "æœã”ã¯ã‚“ã¯ä½•ã‚’é£Ÿã¹ã‚‹ã®ãŒè‰¯ã„ï¼Ÿ"
    ]
    
    for q in questions:
        result = await chain.ainvoke(q)
        print(f"\nâ“ è³ªå•: {q}")
        print(f"ğŸ’¡ å›ç­”: {result}\n")
        print("-" * 60)

# ===== 4. Transform Chain - ãƒ‡ãƒ¼ã‚¿å¤‰æ› =====
async def transform_chain_example():
    """ãƒ‡ãƒ¼ã‚¿å¤‰æ›ã‚’å«ã‚€ãƒã‚§ãƒ¼ãƒ³ã®ä¾‹"""
    print("\n" + "="*60)
    print("4ï¸âƒ£  Transform Chain Example")
    print("="*60)
    
    # ãƒ†ã‚­ã‚¹ãƒˆå‰å‡¦ç†é–¢æ•°
    def preprocess_text(inputs: Dict) -> Dict:
        """ãƒ†ã‚­ã‚¹ãƒˆã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã¨æ­£è¦åŒ–"""
        text = inputs["text"]
        # å°æ–‡å­—åŒ–ã€ç©ºç™½ã®æ­£è¦åŒ–
        cleaned = " ".join(text.lower().split())
        word_count = len(cleaned.split())
        
        return {
            "original": text,
            "cleaned": cleaned,
            "word_count": word_count
        }
    
    # å¾Œå‡¦ç†é–¢æ•°
    def postprocess_result(inputs: Dict) -> Dict:
        """çµæœã®æ•´å½¢"""
        return {
            "summary": inputs["result"],
            "metadata": {
                "original_length": len(inputs["original"]),
                "word_count": inputs["word_count"]
            }
        }
    
    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
    summarize_prompt = ChatPromptTemplate.from_template(
        "ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’30æ–‡å­—ä»¥å†…ã§è¦ç´„ã—ã¦ãã ã•ã„ï¼š\n\n{cleaned}"
    )
    
    # ãƒã‚§ãƒ¼ãƒ³æ§‹ç¯‰
    chain = (
        RunnableLambda(preprocess_text)
        | RunnableParallel(
            original=lambda x: x["original"],
            cleaned=lambda x: x["cleaned"],
            word_count=lambda x: x["word_count"],
            result=(
                RunnableLambda(lambda x: {"cleaned": x["cleaned"]})
                | summarize_prompt
                | get_llm()
                | StrOutputParser()
            )
        )
        | RunnableLambda(postprocess_result)
    )
    
    # å®Ÿè¡Œ
    text = """
    äººå·¥çŸ¥èƒ½ï¼ˆAIï¼‰ã¯ã€ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹ã®ä¸€åˆ†é‡ã§ã€
    äººé–“ã®çŸ¥èƒ½ã‚’æ¨¡å€£ã™ã‚‹ã‚·ã‚¹ãƒ†ãƒ ã®ç ”ç©¶é–‹ç™ºã‚’è¡Œã„ã¾ã™ã€‚
    æ©Ÿæ¢°å­¦ç¿’ã‚„ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ãªã©ã®æŠ€è¡“ãŒå«ã¾ã‚Œã¾ã™ã€‚
    """
    
    result = await chain.ainvoke({"text": text})
    
    print(f"ğŸ“„ å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆé•·: {result['metadata']['original_length']} æ–‡å­—")
    print(f"ğŸ“Š å˜èªæ•°: {result['metadata']['word_count']}")
    print(f"ğŸ“ è¦ç´„: {result['summary']}")
    
    return result

# ===== 5. LCEL Parallel Chain - ä¸¦åˆ—å®Ÿè¡Œ =====
class AnalysisResult(BaseModel):
    """åˆ†æçµæœã®æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿"""
    sentiment: str = Field(description="æ„Ÿæƒ…åˆ†æçµæœï¼ˆpositive/negative/neutralï¼‰")
    category: str = Field(description="ã‚«ãƒ†ã‚´ãƒªåˆ†é¡")
    keywords: List[str] = Field(description="ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆ")

async def parallel_chain_example():
    """è¤‡æ•°ã®ã‚¿ã‚¹ã‚¯ã‚’ä¸¦åˆ—å®Ÿè¡Œã™ã‚‹ä¾‹"""
    print("\n" + "="*60)
    print("5ï¸âƒ£  Parallel Chain Example (LCEL)")
    print("="*60)
    
    # å„åˆ†æç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
    sentiment_prompt = ChatPromptTemplate.from_template(
        "ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã®æ„Ÿæƒ…ã‚’åˆ†æã—ã€positive/negative/neutralã®ã„ãšã‚Œã‹ã§ç­”ãˆã¦ãã ã•ã„ï¼š\n\n{text}"
    )
    
    category_prompt = ChatPromptTemplate.from_template(
        "ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã®ã‚«ãƒ†ã‚´ãƒªã‚’ã€ŒæŠ€è¡“ã€ã€Œãƒ“ã‚¸ãƒã‚¹ã€ã€Œã‚¨ãƒ³ã‚¿ãƒ¡ã€ã€Œãã®ä»–ã€ã‹ã‚‰é¸ã‚“ã§ãã ã•ã„ï¼š\n\n{text}"
    )
    
    keywords_prompt = ChatPromptTemplate.from_template(
        "ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰é‡è¦ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’3ã¤æŠ½å‡ºã—ã€ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã§åˆ—æŒ™ã—ã¦ãã ã•ã„ï¼š\n\n{text}"
    )
    
    summary_prompt = ChatPromptTemplate.from_template(
        "ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ä¸€è¡Œã§è¦ç´„ã—ã¦ãã ã•ã„ï¼š\n\n{text}"
    )
    
    # ä¸¦åˆ—å®Ÿè¡Œãƒã‚§ãƒ¼ãƒ³
    parallel_chain = RunnableParallel(
        sentiment=(sentiment_prompt | get_llm() | StrOutputParser()),
        category=(category_prompt | get_llm() | StrOutputParser()),
        keywords=(keywords_prompt | get_llm() | StrOutputParser()),
        summary=(summary_prompt | get_llm() | StrOutputParser()),
        original=lambda x: x["text"]
    )
    
    # å®Ÿè¡Œ
    text = """
    æ–°ã—ã„AIæŠ€è¡“ã«ã‚ˆã‚Šã€ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã®åŠ¹ç‡ãŒå¤§å¹…ã«å‘ä¸Šã—ã¾ã—ãŸã€‚
    é–‹ç™ºè€…ã¯ã‚ˆã‚Šå‰µé€ çš„ãªä½œæ¥­ã«é›†ä¸­ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã€
    ç”Ÿç”£æ€§ãŒé£›èºçš„ã«æ”¹å–„ã•ã‚Œã¦ã„ã¾ã™ã€‚ç´ æ™´ã‚‰ã—ã„é€²æ­©ã§ã™ï¼
    """
    
    result = await parallel_chain.ainvoke({"text": text})
    
    print(f"ğŸ“Š æ„Ÿæƒ…åˆ†æ: {result['sentiment']}")
    print(f"ğŸ·ï¸  ã‚«ãƒ†ã‚´ãƒª: {result['category']}")
    print(f"ğŸ”‘ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {result['keywords']}")
    print(f"ğŸ“ è¦ç´„: {result['summary']}")
    
    return result

# ===== 6. Streaming Chain - ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡ºåŠ› =====
async def streaming_chain_example():
    """ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡ºåŠ›ã®ãƒã‚§ãƒ¼ãƒ³ä¾‹"""
    print("\n" + "="*60)
    print("6ï¸âƒ£  Streaming Chain Example")
    print("="*60)
    
    prompt = ChatPromptTemplate.from_template(
        "ä»¥ä¸‹ã®ãƒˆãƒ”ãƒƒã‚¯ã«ã¤ã„ã¦ã€300æ–‡å­—ç¨‹åº¦ã§èª¬æ˜ã—ã¦ãã ã•ã„ï¼š{topic}"
    )
    
    chain = prompt | get_llm(streaming=True) | StrOutputParser()
    
    print("ğŸ¬ ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°é–‹å§‹...\n")
    print("ğŸ“ ", end="", flush=True)
    
    full_response = ""
    async for chunk in chain.astream({"topic": "é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã®æœªæ¥"}):
        print(chunk, end="", flush=True)
        full_response += chunk
    
    print("\n\nâœ… ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å®Œäº†")
    return full_response

# ===== 7. Memory Chain - ä¼šè©±å±¥æ­´ã‚’æŒã¤ãƒã‚§ãƒ¼ãƒ³ =====
async def memory_chain_example():
    """ä¼šè©±å±¥æ­´ã‚’ç®¡ç†ã™ã‚‹ãƒã‚§ãƒ¼ãƒ³ä¾‹ï¼ˆLCELç‰ˆï¼‰"""
    print("\n" + "="*60)
    print("7ï¸âƒ£  Memory Chain Example (Conversation)")
    print("="*60)
    
    # ä¼šè©±å±¥æ­´ã‚’ä¿æŒã™ã‚‹ãƒªã‚¹ãƒˆ
    conversation_history = []
    
    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆï¼ˆå±¥æ­´ã‚’å«ã‚€ï¼‰
    prompt = ChatPromptTemplate.from_template(
        """ä»¥ä¸‹ã¯ä»Šã¾ã§ã®ä¼šè©±å±¥æ­´ã§ã™ï¼š
{history}

ãƒ¦ãƒ¼ã‚¶ãƒ¼: {input}
AI:"""
    )
    
    # ãƒã‚§ãƒ¼ãƒ³æ§‹ç¯‰
    chain = prompt | get_llm() | StrOutputParser()
    
    # ä¼šè©±ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
    conversations = [
        "ã“ã‚“ã«ã¡ã¯ï¼ç§ã®åå‰ã¯å¤ªéƒã§ã™ã€‚",
        "ç§ã®å¥½ããªãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°è¨€èªã¯Pythonã§ã™ã€‚",
        "ç§ã®åå‰ã‚’è¦šãˆã¦ã„ã¾ã™ã‹ï¼Ÿ",
        "ç§ã®å¥½ããªè¨€èªã¯ä½•ã§ã—ãŸã£ã‘ï¼Ÿ"
    ]
    
    for user_input in conversations:
        # å±¥æ­´ã‚’æ–‡å­—åˆ—ã«å¤‰æ›
        history_text = "\n".join([
            f"ãƒ¦ãƒ¼ã‚¶ãƒ¼: {h['user']}\nAI: {h['ai']}"
            for h in conversation_history
        ]) if conversation_history else "ï¼ˆã¾ã ä¼šè©±å±¥æ­´ã¯ã‚ã‚Šã¾ã›ã‚“ï¼‰"
        
        # AIã®å¿œç­”ã‚’å–å¾—
        response = await chain.ainvoke({
            "history": history_text,
            "input": user_input
        })
        
        # å±¥æ­´ã«è¿½åŠ 
        conversation_history.append({
            "user": user_input,
            "ai": response
        })
        
        print(f"ğŸ‘¤ ãƒ¦ãƒ¼ã‚¶ãƒ¼: {user_input}")
        print(f"ğŸ¤– AI: {response}\n")
        print("-" * 60)

# ===== 8. Custom Chain - ã‚«ã‚¹ã‚¿ãƒ ãƒã‚§ãƒ¼ãƒ³ =====
async def custom_chain_example():
    """ã‚«ã‚¹ã‚¿ãƒ ãƒ­ã‚¸ãƒƒã‚¯ã‚’å«ã‚€ãƒã‚§ãƒ¼ãƒ³ä¾‹"""
    print("\n" + "="*60)
    print("8ï¸âƒ£  Custom Chain Example")
    print("="*60)
    
    # ã‚«ã‚¹ã‚¿ãƒ å‡¦ç†é–¢æ•°
    async def validate_and_enhance(inputs: Dict) -> Dict:
        """å…¥åŠ›ã‚’æ¤œè¨¼ã—ã¦æ‹¡å¼µ"""
        query = inputs["query"]
        
        # ç°¡å˜ãªãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
        if len(query) < 5:
            return {
                "error": "ã‚¯ã‚¨ãƒªãŒçŸ­ã™ãã¾ã™",
                "enhanced_query": None
            }
        
        # ã‚¯ã‚¨ãƒªã®æ‹¡å¼µ
        enhanced = f"{query}ï¼ˆå…·ä½“ä¾‹ã‚„å®Ÿç”¨çš„ãªæƒ…å ±ã‚’å«ã‚ã¦è©³ã—ãï¼‰"
        
        return {
            "error": None,
            "enhanced_query": enhanced,
            "original_query": query
        }
    
    # æ¡ä»¶ä»˜ãå®Ÿè¡Œ
    def execute_if_valid(inputs: Dict) -> Any:
        if inputs["error"]:
            return RunnableLambda(lambda x: f"ã‚¨ãƒ©ãƒ¼: {x['error']}")
        else:
            prompt = ChatPromptTemplate.from_template("{enhanced_query}")
            return prompt | get_llm() | StrOutputParser()
    
    # ã‚«ã‚¹ã‚¿ãƒ ãƒã‚§ãƒ¼ãƒ³
    chain = (
        RunnableLambda(validate_and_enhance)
        | RunnableLambda(execute_if_valid)
    )
    
    # ãƒ†ã‚¹ãƒˆ
    test_queries = ["AI", "æ©Ÿæ¢°å­¦ç¿’ã¨ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ã®é•ã„"]
    
    for query in test_queries:
        print(f"\nâ“ ã‚¯ã‚¨ãƒª: {query}")
        result = await chain.ainvoke({"query": query})
        print(f"ğŸ’¡ çµæœ: {result}")
        print("-" * 60)

# ===== ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ =====
async def main():
    """å…¨ã¦ã®ã‚µãƒ³ãƒ—ãƒ«ã‚’å®Ÿè¡Œ"""
    print("\n" + "ğŸŒŸ"*30)
    print("LangChain ãƒã‚§ãƒ¼ãƒ³ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ ã‚µãƒ³ãƒ—ãƒ«é›†")
    print("ğŸŒŸ"*30)
    
    examples = [
        ("Simple Chain", simple_chain_example),
        ("Sequential Chain", sequential_chain_example),
        ("Router Chain", router_chain_example),
        ("Transform Chain", transform_chain_example),
        ("Parallel Chain", parallel_chain_example),
        ("Streaming Chain", streaming_chain_example),
        ("Memory Chain", memory_chain_example),
        ("Custom Chain", custom_chain_example),
    ]
    
    print("\nå®Ÿè¡Œã™ã‚‹ã‚µãƒ³ãƒ—ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„:")
    for i, (name, _) in enumerate(examples, 1):
        print(f"{i}. {name}")
    print("0. å…¨ã¦å®Ÿè¡Œ")
    
    try:
        choice = input("\nç•ªå·ã‚’å…¥åŠ› (0-8): ").strip()
        
        if choice == "0":
            for name, func in examples:
                await func()
        elif choice.isdigit() and 1 <= int(choice) <= len(examples):
            _, func = examples[int(choice) - 1]
            await func()
        else:
            print("âŒ ç„¡åŠ¹ãªé¸æŠã§ã™")
            return
            
    except KeyboardInterrupt:
        print("\n\nâš ï¸  å®Ÿè¡ŒãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
    except Exception as e:
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        traceback.print_exc()
    
    print("\n" + "ğŸ‰"*30)
    print("ã‚µãƒ³ãƒ—ãƒ«å®Ÿè¡Œå®Œäº†ï¼")
    print("ğŸ‰"*30 + "\n")

if __name__ == "__main__":
    asyncio.run(main())
