"""
LangChain RAG (Retrieval-Augmented Generation) å®Œå…¨ã‚µãƒ³ãƒ—ãƒ«

ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ã€embeddingã‚’ä½¿ç”¨ã—ãŸRAGã‚·ã‚¹ãƒ†ãƒ ã®å®Ÿè£…ä¾‹ã§ã™ï¼š
1. ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®èª­ã¿è¾¼ã¿ã¨ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²
2. ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã®æ§‹ç¯‰
3. é¡ä¼¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ¤œç´¢
4. LLMã¨çµ„ã¿åˆã‚ã›ãŸè³ªå•å¿œç­”
"""

import asyncio
from typing import List
from langchain_community.embeddings import OllamaEmbeddings
from langchain_openai import ChatOpenAI
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_text_splitters import RecursiveCharacterTextSplitter

# ===== è¨­å®š =====
def get_embeddings():
    """Embedding ãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—"""
    return OllamaEmbeddings(
        base_url="http://localhost:11434",
        model="mxbai-embed-large"
    )

def get_llm():
    """LLMãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—"""
    return ChatOpenAI(
        openai_api_base="http://localhost:11434/v1",
        temperature=0.7,
        openai_api_key="EMPTY",
        model="qwen3:8b"
    )

# ===== ã‚µãƒ³ãƒ—ãƒ«ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ =====
SAMPLE_DOCUMENTS = [
    """
    Python ã¯ã€1991å¹´ã«Guido van Rossumã«ã‚ˆã£ã¦é–‹ç™ºã•ã‚ŒãŸãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°è¨€èªã§ã™ã€‚
    èª­ã¿ã‚„ã™ãã€æ›¸ãã‚„ã™ã„æ§‹æ–‡ãŒç‰¹å¾´ã§ã€åˆå¿ƒè€…ã«ã‚‚æ‰±ã„ã‚„ã™ã„è¨€èªã¨ã—ã¦åºƒãä½¿ã‚ã‚Œã¦ã„ã¾ã™ã€‚
    Webé–‹ç™ºã€ãƒ‡ãƒ¼ã‚¿åˆ†æã€æ©Ÿæ¢°å­¦ç¿’ã€è‡ªå‹•åŒ–ãªã©ã€æ§˜ã€…ãªåˆ†é‡ã§æ´»ç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚
    """,
    """
    æ©Ÿæ¢°å­¦ç¿’ã¯äººå·¥çŸ¥èƒ½ã®ä¸€åˆ†é‡ã§ã€ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãŒãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å­¦ç¿’ã™ã‚‹æŠ€è¡“ã§ã™ã€‚
    æ•™å¸«ã‚ã‚Šå­¦ç¿’ã€æ•™å¸«ãªã—å­¦ç¿’ã€å¼·åŒ–å­¦ç¿’ã®3ã¤ã®ä¸»è¦ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒãŒã‚ã‚Šã¾ã™ã€‚
    ç”»åƒèªè­˜ã€éŸ³å£°èªè­˜ã€è‡ªç„¶è¨€èªå‡¦ç†ãªã©ã®å¿œç”¨ãŒã‚ã‚Šã¾ã™ã€‚
    """,
    """
    æ·±å±¤å­¦ç¿’ã¯ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ä½¿ç”¨ã—ãŸæ©Ÿæ¢°å­¦ç¿’ã®æ‰‹æ³•ã§ã™ã€‚
    å¤šå±¤ã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«ã‚ˆã‚Šã€è¤‡é›‘ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’ã§ãã¾ã™ã€‚
    CNNã¯ç”»åƒå‡¦ç†ã«ã€RNNã¯æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã«ã€Transformerã¯è‡ªç„¶è¨€èªå‡¦ç†ã«ä½¿ã‚ã‚Œã¾ã™ã€‚
    """,
    """
    è‡ªç„¶è¨€èªå‡¦ç†ï¼ˆNLPï¼‰ã¯ã€äººé–“ã®è¨€èªã‚’ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã§å‡¦ç†ã™ã‚‹æŠ€è¡“ã§ã™ã€‚
    å½¢æ…‹ç´ è§£æã€æ§‹æ–‡è§£æã€æ„å‘³è§£æãªã©ã®åŸºæœ¬æŠ€è¡“ãŒã‚ã‚Šã¾ã™ã€‚
    æ©Ÿæ¢°ç¿»è¨³ã€æ–‡ç« è¦ç´„ã€æ„Ÿæƒ…åˆ†æã€è³ªå•å¿œç­”ã‚·ã‚¹ãƒ†ãƒ ãªã©ã«å¿œç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚
    """,
    """
    LangChainã¯ã€å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMï¼‰ã‚’ä½¿ã£ãŸã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³é–‹ç™ºã‚’
    ç°¡å˜ã«ã™ã‚‹ãŸã‚ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã™ã€‚ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç®¡ç†ã€ãƒã‚§ãƒ¼ãƒ³æ§‹ç¯‰ã€
    ãƒ¡ãƒ¢ãƒªç®¡ç†ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæ©Ÿèƒ½ãªã©ã‚’æä¾›ã—ã¾ã™ã€‚
    """,
    """
    RAGï¼ˆRetrieval-Augmented Generationï¼‰ã¯ã€æ¤œç´¢ã¨ç”Ÿæˆã‚’çµ„ã¿åˆã‚ã›ãŸæŠ€è¡“ã§ã™ã€‚
    å¤–éƒ¨ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ã‹ã‚‰é–¢é€£æƒ…å ±ã‚’æ¤œç´¢ã—ã€ãã‚Œã‚’å…ƒã«LLMãŒå›ç­”ã‚’ç”Ÿæˆã—ã¾ã™ã€‚
    ã“ã‚Œã«ã‚ˆã‚Šã€LLMã®çŸ¥è­˜ã‚’æœ€æ–°æƒ…å ±ã§è£œå®Œã§ãã¾ã™ã€‚
    """,
    """
    ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¯ã€é«˜æ¬¡å…ƒãƒ™ã‚¯ãƒˆãƒ«ã®åŠ¹ç‡çš„ãªä¿å­˜ã¨æ¤œç´¢ã‚’è¡Œã†ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã§ã™ã€‚
    embeddingãƒ™ã‚¯ãƒˆãƒ«ã‚’ä¿å­˜ã—ã€é¡ä¼¼åº¦æ¤œç´¢ã‚’é«˜é€Ÿã«å®Ÿè¡Œã§ãã¾ã™ã€‚
    FAISSã€Chromaã€Pineconeã€Weaviateãªã©ãŒä»£è¡¨çš„ãªå®Ÿè£…ã§ã™ã€‚
    """,
    """
    Transformerã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¯ã€2017å¹´ã«ç™ºè¡¨ã•ã‚ŒãŸé©æ–°çš„ãªãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§ã™ã€‚
    Self-Attentionãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã«ã‚ˆã‚Šã€é•·è·é›¢ä¾å­˜é–¢ä¿‚ã‚’åŠ¹æœçš„ã«æ‰ãˆã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚
    BERTã€GPTã€T5ãªã©ã€å¤šãã®æœ€æ–°ãƒ¢ãƒ‡ãƒ«ã®ãƒ™ãƒ¼ã‚¹ã¨ãªã£ã¦ã„ã¾ã™ã€‚
    """
]

# ===== 1. ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæº–å‚™ã¨ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢æ§‹ç¯‰ =====
async def build_vector_store():
    """ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’æ§‹ç¯‰"""
    print("\n" + "="*70)
    print("1ï¸âƒ£  ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã®æ§‹ç¯‰")
    print("="*70)
    
    # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ä½œæˆ
    documents = [
        Document(page_content=text.strip(), metadata={"source": f"doc_{i}"})
        for i, text in enumerate(SAMPLE_DOCUMENTS, 1)
    ]
    
    print(f"ğŸ“š ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°: {len(documents)}")
    
    # ãƒ†ã‚­ã‚¹ãƒˆåˆ†å‰²å™¨
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=200,
        chunk_overlap=50,
        separators=["\n\n", "\n", "ã€‚", "ã€", " ", ""]
    )
    
    # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’åˆ†å‰²
    splits = text_splitter.split_documents(documents)
    print(f"ğŸ“„ åˆ†å‰²å¾Œã®ãƒãƒ£ãƒ³ã‚¯æ•°: {len(splits)}")
    
    # Embeddingãƒ¢ãƒ‡ãƒ«
    embeddings = get_embeddings()
    
    # ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã®ä½œæˆ
    print("ğŸ”„ ãƒ™ã‚¯ãƒˆãƒ«åŒ–ä¸­...")
    vectorstore = await FAISS.afrom_documents(splits, embeddings)
    print("âœ… ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢æ§‹ç¯‰å®Œäº†")
    
    return vectorstore

# ===== 2. é¡ä¼¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ¤œç´¢ =====
async def similarity_search_demo(vectorstore):
    """é¡ä¼¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ¤œç´¢ã®ãƒ‡ãƒ¢"""
    print("\n" + "="*70)
    print("2ï¸âƒ£  é¡ä¼¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ¤œç´¢")
    print("="*70)
    
    queries = [
        "Pythonã«ã¤ã„ã¦æ•™ãˆã¦",
        "æ©Ÿæ¢°å­¦ç¿’ã®ç¨®é¡ã¯ï¼Ÿ",
        "RAGã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ"
    ]
    
    for query in queries:
        print(f"\nğŸ” ã‚¯ã‚¨ãƒª: {query}")
        
        # é¡ä¼¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ¤œç´¢ï¼ˆä¸Šä½3ä»¶ï¼‰
        docs = await vectorstore.asimilarity_search(query, k=3)
        
        print("ğŸ“– æ¤œç´¢çµæœ:")
        for i, doc in enumerate(docs, 1):
            content = doc.page_content.replace('\n', ' ')[:100]
            print(f"  {i}. {content}...")
            print(f"     (ã‚½ãƒ¼ã‚¹: {doc.metadata.get('source', 'unknown')})")

# ===== 3. ã‚¹ã‚³ã‚¢ä»˜ãæ¤œç´¢ =====
async def similarity_search_with_score_demo(vectorstore):
    """ã‚¹ã‚³ã‚¢ä»˜ãé¡ä¼¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ¤œç´¢"""
    print("\n" + "="*70)
    print("3ï¸âƒ£  ã‚¹ã‚³ã‚¢ä»˜ãæ¤œç´¢")
    print("="*70)
    
    query = "ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å¿œç”¨"
    print(f"ğŸ” ã‚¯ã‚¨ãƒª: {query}\n")
    
    # ã‚¹ã‚³ã‚¢ä»˜ãæ¤œç´¢
    docs_with_scores = await vectorstore.asimilarity_search_with_score(query, k=5)
    
    print("ğŸ“Š æ¤œç´¢çµæœï¼ˆé¡ä¼¼åº¦ã‚¹ã‚³ã‚¢ä»˜ãï¼‰:")
    for i, (doc, score) in enumerate(docs_with_scores, 1):
        content = doc.page_content.replace('\n', ' ')[:80]
        print(f"  {i}. ã‚¹ã‚³ã‚¢: {score:.4f}")
        print(f"     å†…å®¹: {content}...")
        print()

# ===== 4. RAGãƒã‚§ãƒ¼ãƒ³ï¼šè³ªå•å¿œç­”ã‚·ã‚¹ãƒ†ãƒ  =====
async def rag_chain_demo(vectorstore):
    """RAGã‚’ä½¿ã£ãŸè³ªå•å¿œç­”ã‚·ã‚¹ãƒ†ãƒ """
    print("\n" + "="*70)
    print("4ï¸âƒ£  RAGãƒã‚§ãƒ¼ãƒ³ï¼šè³ªå•å¿œç­”ã‚·ã‚¹ãƒ†ãƒ ")
    print("="*70)
    
    # Retriever ã®ä½œæˆ
    retriever = vectorstore.as_retriever(
        search_type="similarity",
        search_kwargs={"k": 3}
    )
    
    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
    template = """ä»¥ä¸‹ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ä½¿ç”¨ã—ã¦è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ã€‚
ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«æƒ…å ±ãŒãªã„å ´åˆã¯ã€ã€Œã‚ã‹ã‚Šã¾ã›ã‚“ã€ã¨ç­”ãˆã¦ãã ã•ã„ã€‚

ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ:
{context}

è³ªå•: {question}

å›ç­”:"""
    
    prompt = ChatPromptTemplate.from_template(template)
    
    # RAGãƒã‚§ãƒ¼ãƒ³ã®æ§‹ç¯‰
    def format_docs(docs):
        return "\n\n".join(doc.page_content for doc in docs)
    
    rag_chain = (
        {"context": retriever | format_docs, "question": RunnablePassthrough()}
        | prompt
        | get_llm()
        | StrOutputParser()
    )
    
    # è³ªå•ãƒªã‚¹ãƒˆ
    questions = [
        "Pythonã¯ã„ã¤é–‹ç™ºã•ã‚Œã¾ã—ãŸã‹ï¼Ÿ",
        "æ©Ÿæ¢°å­¦ç¿’ã«ã¯ã©ã‚“ãªç¨®é¡ãŒã‚ã‚Šã¾ã™ã‹ï¼Ÿ",
        "RAGã®ä»•çµ„ã¿ã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚",
        "LangChainã¯ä½•ã«ä½¿ã‚ã‚Œã¾ã™ã‹ï¼Ÿ"
    ]
    
    for question in questions:
        print(f"\nâ“ è³ªå•: {question}")
        
        # æ¤œç´¢ã•ã‚Œã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’è¡¨ç¤º
        retrieved_docs = await retriever.ainvoke(question)
        print(f"ğŸ“š å‚ç…§ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°: {len(retrieved_docs)}")
        
        # å›ç­”ç”Ÿæˆ
        answer = await rag_chain.ainvoke(question)
        print(f"ğŸ’¡ å›ç­”: {answer}")
        print("-" * 70)

# ===== 5. ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°RAG =====
async def streaming_rag_demo(vectorstore):
    """ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡ºåŠ›ã§ã®RAG"""
    print("\n" + "="*70)
    print("5ï¸âƒ£  ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°RAG")
    print("="*70)
    
    retriever = vectorstore.as_retriever(search_kwargs={"k": 2})
    
    template = """ä»¥ä¸‹ã®æƒ…å ±ã‚’å‚è€ƒã«ã€è³ªå•ã«è©³ã—ãç­”ãˆã¦ãã ã•ã„ã€‚

å‚è€ƒæƒ…å ±:
{context}

è³ªå•: {question}

å›ç­”:"""
    
    prompt = ChatPromptTemplate.from_template(template)
    
    def format_docs(docs):
        return "\n\n".join(doc.page_content for doc in docs)
    
    # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ç”¨LLM
    streaming_llm = ChatOpenAI(
        openai_api_base="http://localhost:11434/v1",
        streaming=True,
        temperature=0.7,
        openai_api_key="EMPTY",
        model="qwen3:8b"
    )
    
    rag_chain = (
        {"context": retriever | format_docs, "question": RunnablePassthrough()}
        | prompt
        | streaming_llm
        | StrOutputParser()
    )
    
    question = "æ·±å±¤å­¦ç¿’ã¨Transformerã«ã¤ã„ã¦è©³ã—ãæ•™ãˆã¦ãã ã•ã„ã€‚"
    print(f"â“ è³ªå•: {question}\n")
    print("ğŸ’¡ å›ç­”ï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ï¼‰:")
    print("   ", end="", flush=True)
    
    async for chunk in rag_chain.astream(question):
        print(chunk, end="", flush=True)
    
    print("\n\nâœ… ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å®Œäº†")

# ===== 6. MMRï¼ˆMaximum Marginal Relevanceï¼‰æ¤œç´¢ =====
async def mmr_search_demo(vectorstore):
    """å¤šæ§˜æ€§ã‚’è€ƒæ…®ã—ãŸæ¤œç´¢"""
    print("\n" + "="*70)
    print("6ï¸âƒ£  MMRæ¤œç´¢ï¼ˆå¤šæ§˜æ€§è€ƒæ…®ï¼‰")
    print("="*70)
    
    query = "æ©Ÿæ¢°å­¦ç¿’ã®æŠ€è¡“"
    print(f"ğŸ” ã‚¯ã‚¨ãƒª: {query}\n")
    
    # é€šå¸¸ã®é¡ä¼¼åº¦æ¤œç´¢
    print("ğŸ“– é€šå¸¸ã®é¡ä¼¼åº¦æ¤œç´¢:")
    normal_docs = await vectorstore.asimilarity_search(query, k=4)
    for i, doc in enumerate(normal_docs, 1):
        content = doc.page_content.replace('\n', ' ')[:60]
        print(f"  {i}. {content}...")
    
    # MMRæ¤œç´¢ï¼ˆå¤šæ§˜æ€§ã‚’è€ƒæ…®ï¼‰
    print("\nğŸ“– MMRæ¤œç´¢ï¼ˆå¤šæ§˜æ€§é‡è¦–ï¼‰:")
    mmr_docs = await vectorstore.amax_marginal_relevance_search(
        query, 
        k=4,
        fetch_k=10  # å€™è£œã¨ã—ã¦10ä»¶å–å¾—ã—ã€ãã®ä¸­ã‹ã‚‰å¤šæ§˜ãª4ä»¶ã‚’é¸æŠ
    )
    for i, doc in enumerate(mmr_docs, 1):
        content = doc.page_content.replace('\n', ' ')[:60]
        print(f"  {i}. {content}...")
    
    print("\nğŸ’¡ MMRã¯é¡ä¼¼æ€§ã¨å¤šæ§˜æ€§ã®ãƒãƒ©ãƒ³ã‚¹ã‚’å–ã£ãŸçµæœã‚’è¿”ã—ã¾ã™")

# ===== ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ =====
async def main():
    """å…¨ã¦ã®ã‚µãƒ³ãƒ—ãƒ«ã‚’å®Ÿè¡Œ"""
    print("\n" + "ğŸŒŸ"*35)
    print("LangChain RAG å®Œå…¨ã‚µãƒ³ãƒ—ãƒ«")
    print("ğŸŒŸ"*35)
    
    # ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã®æ§‹ç¯‰
    print("\nâš™ï¸  åˆæœŸåŒ–ä¸­...")
    vectorstore = await build_vector_store()
    
    examples = [
        ("é¡ä¼¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ¤œç´¢", lambda: similarity_search_demo(vectorstore)),
        ("ã‚¹ã‚³ã‚¢ä»˜ãæ¤œç´¢", lambda: similarity_search_with_score_demo(vectorstore)),
        ("RAGãƒã‚§ãƒ¼ãƒ³ï¼šè³ªå•å¿œç­”", lambda: rag_chain_demo(vectorstore)),
        ("ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°RAG", lambda: streaming_rag_demo(vectorstore)),
        ("MMRæ¤œç´¢", lambda: mmr_search_demo(vectorstore)),
    ]
    
    print("\nå®Ÿè¡Œã™ã‚‹ã‚µãƒ³ãƒ—ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„:")
    for i, (name, _) in enumerate(examples, 1):
        print(f"{i}. {name}")
    print("0. å…¨ã¦å®Ÿè¡Œ")
    
    try:
        choice = input("\nç•ªå·ã‚’å…¥åŠ› (0-5): ").strip()
        
        if choice == "0":
            for name, func in examples:
                await func()
        elif choice.isdigit() and 1 <= int(choice) <= len(examples):
            _, func = examples[int(choice) - 1]
            await func()
        else:
            print("âŒ ç„¡åŠ¹ãªé¸æŠã§ã™")
            return
            
    except KeyboardInterrupt:
        print("\n\nâš ï¸  å®Ÿè¡ŒãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
    except Exception as e:
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        traceback.print_exc()
    
    print("\n" + "ğŸ‰"*35)
    print("ã‚µãƒ³ãƒ—ãƒ«å®Ÿè¡Œå®Œäº†ï¼")
    print("ğŸ‰"*35 + "\n")

if __name__ == "__main__":
    asyncio.run(main())
